# PySpark Sales Data Analysis Project

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5+-orange.svg)](https://spark.apache.org/docs/latest/api/python/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Má»™t project PySpark hoÃ n chá»‰nh vÃ  chuyÃªn nghiá»‡p Ä‘á»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u bÃ¡n hÃ ng vá»›i cÃ¡c tÃ­nh nÄƒng tá»« cÆ¡ báº£n Ä‘áº¿n nÃ¢ng cao.

## ğŸ“‹ Má»¥c Lá»¥c

- [TÃ­nh NÄƒng](#tÃ­nh-nÄƒng)
- [Cáº¥u TrÃºc Project](#cáº¥u-trÃºc-project)
- [YÃªu Cáº§u Há»‡ Thá»‘ng](#yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Äáº·t](#cÃ i-Ä‘áº·t)
- [Sá»­ Dá»¥ng](#sá»­-dá»¥ng)
- [Cáº¥u TrÃºc Code](#cáº¥u-trÃºc-code)
- [TÃ i Liá»‡u](#tÃ i-liá»‡u)
- [ÄÃ³ng GÃ³p](#Ä‘Ã³ng-gÃ³p)
- [License](#license)

## âœ¨ TÃ­nh NÄƒng

### CÆ¡ Báº£n
- âœ… Äá»c/Ghi dá»¯ liá»‡u CSV, Parquet
- âœ… DataFrame operations (filter, select, groupBy)
- âœ… Aggregation functions (sum, avg, count, min, max)
- âœ… Join operations (inner, left, right, outer)
- âœ… Xá»­ lÃ½ missing values

### NÃ¢ng Cao
- âœ… Window Functions (ranking, running totals, LAG/LEAD)
- âœ… User Defined Functions (UDF)
- âœ… Broadcast variables vÃ  Accumulators
- âœ… Partitioning strategies
- âœ… Caching vÃ  persistence
- âœ… Performance optimization vá»›i Adaptive Query Execution (AQE)

### PhÃ¢n TÃ­ch Dá»¯ Liá»‡u
- âœ… PhÃ¢n tÃ­ch doanh thu theo phÃ²ng ban
- âœ… Top sáº£n pháº©m vÃ  nhÃ¢n viÃªn
- âœ… Thá»‘ng kÃª theo category
- âœ… PhÃ¢n tÃ­ch theo thá»i gian (thÃ¡ng, quÃ½)
- âœ… Running totals vÃ  so sÃ¡nh thÃ¡ng trÆ°á»›c

## ğŸ“ Cáº¥u TrÃºc Project

```
.
â”œâ”€â”€ src/
â”‚   â””â”€â”€ pyspark_project/          # Package chÃ­nh
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config/               # Cáº¥u hÃ¬nh
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ spark_config.py   # Cáº¥u hÃ¬nh Spark
â”‚       â”‚   â””â”€â”€ paths.py          # Quáº£n lÃ½ Ä‘Æ°á»ng dáº«n
â”‚       â”œâ”€â”€ data/                 # Data generation
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ generator.py      # Táº¡o dá»¯ liá»‡u máº«u
â”‚       â”œâ”€â”€ analysis/             # PhÃ¢n tÃ­ch
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ data_loader.py    # Load dá»¯ liá»‡u
â”‚       â”‚   â”œâ”€â”€ data_transformer.py  # Transform
â”‚       â”‚   â”œâ”€â”€ aggregator.py     # Aggregation
â”‚       â”‚   â””â”€â”€ window_analyzer.py  # Window functions
â”‚       â””â”€â”€ utils/                # Utilities
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ dataframe_utils.py
â”œâ”€â”€ scripts/                      # Scripts cháº¡y
â”‚   â”œâ”€â”€ generate_data.py
â”‚   â””â”€â”€ run_analysis.py
â”œâ”€â”€ data/                         # Dá»¯ liá»‡u
â”‚   â”œâ”€â”€ raw/                      # Dá»¯ liá»‡u thÃ´
â”‚   â””â”€â”€ processed/                # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
â”œâ”€â”€ output/                       # Káº¿t quáº£
â”‚   â”œâ”€â”€ parquet/                  # Output Parquet
â”‚   â”œâ”€â”€ csv/                      # Output CSV
â”‚   â””â”€â”€ reports/                  # BÃ¡o cÃ¡o
â”œâ”€â”€ docs/                         # TÃ i liá»‡u
â”‚   â”œâ”€â”€ pyspark-co-ban.md
â”‚   â”œâ”€â”€ pyspark-nang-cao.md
â”‚   â””â”€â”€ *.pdf
â”œâ”€â”€ tests/                        # Unit tests
â”œâ”€â”€ config/                       # File cáº¥u hÃ¬nh
â”œâ”€â”€ setup.py                      # Setup script
â”œâ”€â”€ pyproject.toml               # Project metadata
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ README.md                    # File nÃ y
â””â”€â”€ LICENSE                      # License
```

## ğŸ”§ YÃªu Cáº§u Há»‡ Thá»‘ng

- **Python**: 3.8 hoáº·c cao hÆ¡n
- **Java**: 8 hoáº·c cao hÆ¡n (báº¯t buá»™c cho PySpark)
- **RAM**: Tá»‘i thiá»ƒu 4GB (khuyáº¿n nghá»‹ 8GB+)
- **Disk**: ~500MB cho cÃ i Ä‘áº·t

## ğŸ“¦ CÃ i Äáº·t

### 1. Clone repository

```bash
git clone https://github.com/trandaitai327/spark_document.git
cd Spark
```

### 2. Táº¡o virtual environment (khuyáº¿n nghá»‹)

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

Hoáº·c cÃ i Ä‘áº·t nhÆ° má»™t package:

```bash
pip install -e .
```

### 4. Kiá»ƒm tra cÃ i Ä‘áº·t

```bash
python -c "import pyspark; print(pyspark.__version__)"
java -version
```

## ğŸš€ Sá»­ Dá»¥ng

### 1. Táº¡o dá»¯ liá»‡u máº«u

```bash
python scripts/generate_data.py
```

Hoáº·c:

```bash
cd scripts
python generate_data.py
```

Lá»‡nh nÃ y sáº½ táº¡o cÃ¡c file CSV máº«u trong thÆ° má»¥c `data/raw/`:
- `employees.csv` - 100 nhÃ¢n viÃªn
- `products.csv` - 50 sáº£n pháº©m
- `sales.csv` - 10,000 giao dá»‹ch bÃ¡n hÃ ng
- `departments.csv` - 6 phÃ²ng ban

### 2. Cháº¡y phÃ¢n tÃ­ch

```bash
python scripts/run_analysis.py
```

Hoáº·c:

```bash
cd scripts
python run_analysis.py
```

Script nÃ y sáº½ thá»±c hiá»‡n:
1. Äá»c dá»¯ liá»‡u tá»« CSV
2. Transform vÃ  enrich dá»¯ liá»‡u
3. Thá»±c hiá»‡n cÃ¡c phÃ©p aggregation
4. Sá»­ dá»¥ng Window Functions
5. LÆ°u káº¿t quáº£ ra Parquet vÃ  CSV

### 3. Xem káº¿t quáº£

Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `output/`:
- `parquet/` - CÃ¡c file Parquet
- `csv/` - CÃ¡c file CSV (top products, employees, departments)
- `reports/` - BÃ¡o cÃ¡o (náº¿u cÃ³)

## ğŸ“š Cáº¥u TrÃºc Code

### Config Module

```python
from pyspark_project.config import create_spark_session, get_data_paths

spark = create_spark_session("My App")
paths = get_data_paths()
```

### Data Generation

```python
from pyspark_project.data import DataGenerator

generator = DataGenerator()
generator.generate_all(n_employees=100, n_products=50, n_sales=10000)
```

### Analysis

```python
from pyspark_project.analysis import DataLoader, DataTransformer, Aggregator

loader = DataLoader(spark)
employees, products, sales, departments = loader.load_all()

transformer = DataTransformer()
sales_enriched = transformer.enrich_sales(sales, employees, products)

aggregator = Aggregator()
top_products = aggregator.top_products(sales_enriched, limit=10)
```

## ğŸ“– TÃ i Liá»‡u

TÃ i liá»‡u chi tiáº¿t vá» PySpark Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `docs/`:

- `pyspark-co-ban.md` - HÆ°á»›ng dáº«n PySpark cÆ¡ báº£n
- `pyspark-nang-cao.md` - HÆ°á»›ng dáº«n PySpark nÃ¢ng cao
- CÃ¡c file PDF tÆ°Æ¡ng á»©ng

## âš™ï¸ Cáº¥u HÃ¬nh

### Environment Variables

Báº¡n cÃ³ thá»ƒ cáº¥u hÃ¬nh Spark thÃ´ng qua environment variables:

```bash
export SPARK_MASTER="local[*]"
export SPARK_LOG_LEVEL="WARN"
export SPARK_SQL_SHUFFLE_PARTITIONS="200"
```

### Spark Config

Chá»‰nh sá»­a `src/pyspark_project/config/spark_config.py` Ä‘á»ƒ tÃ¹y chá»‰nh cáº¥u hÃ¬nh Spark.

## ğŸ§ª Testing

```bash
pytest tests/
```

## ğŸ¤ ÄÃ³ng GÃ³p

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the project
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

Äáº¡i TÃ i - trandaitai327@gmail.com

## ğŸ™ Acknowledgments

- Apache Spark community
- PySpark documentation
- All contributors

---

**Note**: Project nÃ y dÃ¹ng cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  demo. Äiá»u chá»‰nh cáº¥u hÃ¬nh vÃ  code theo nhu cáº§u thá»±c táº¿ cá»§a báº¡n.
